<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>file_1648578500756</title><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 h1 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 28pt; }
 h2 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt; }
 h3 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9.5pt; }
 p { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; margin:0pt; }
 .s1 { color: black; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 9.5pt; }
 .s2 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: underline; font-size: 11pt; }
</style></head><body><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 14pt;padding-left: 231pt;text-indent: 0pt;text-align: left;">Table of Contents</h1><p style="text-indent: 0pt;text-align: left;"/><h2 style="padding-top: 44pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Preface                                         xxiii</h2><p style="text-indent: 0pt;text-align: left;"/><h2 style="padding-top: 20pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">Chapter 1: Giving Computers the Ability to Learn from Data            1</h2><h3 style="padding-top: 10pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Building intelligent machines to transform data into knowledge                  1</h3><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">The three different types of machine learning                            2</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Making predictions about the future with supervised learning • 3</p><p class="s1" style="padding-top: 2pt;padding-left: 49pt;text-indent: 0pt;line-height: 121%;text-align: left;">Classification for predicting class labels • 4 Regression for predicting continuous outcomes • 5</p><p style="padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Solving interactive problems with reinforcement learning • 6 Discovering hidden structures with unsupervised learning • 7</p><p class="s1" style="padding-left: 49pt;text-indent: 0pt;line-height: 121%;text-align: left;">Finding subgroups with clustering • 8 Dimensionality reduction for data compression • 8</p><h3 style="padding-left: 31pt;text-indent: 0pt;text-align: left;">Introduction to the basic terminology and notations                         9</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Notation and conventions used in this book • 9 Machine learning terminology • 11</p><h3 style="padding-left: 30pt;text-indent: 0pt;text-align: left;">A roadmap for building machine learning systems                         12</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Preprocessing – getting data into shape • 13 Training and selecting a predictive model • 13</p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;">Evaluating models and predicting unseen data instances • 14</p><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Using Python for machine learning                                  14</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Installing Python and packages from the Python Package Index • 14 Using the Anaconda Python distribution and package manager • 15</p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;">Packages for scientific computing, data science, and machine learning • 16</p><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Summary                                                17</h3><p class="s2" style="padding-top: 5pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Chapter 2: Training Simple Machine Learning Algorithms for Classification &nbsp;&nbsp; 19</p><h3 style="padding-top: 9pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Artificial neurons – a brief glimpse into the early history of machine learning          19</h3><p style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;line-height: 121%;text-align: left;">The formal definition of an artificial neuron • 20 The perceptron learning rule • 22</p><h3 style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Implementing a perceptron learning algorithm in Python                     25</h3><p style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">An object-oriented perceptron API • 25</p><p style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">Training a perceptron model on the Iris dataset • 29</p><h3 style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Adaptive linear neurons and the convergence of learning                     35</h3><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;line-height: 121%;text-align: left;">Minimizing loss functions with gradient descent • 37 Implementing Adaline in Python • 39</p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;">Improving gradient descent through feature scaling • 43</p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Large-scale machine learning and stochastic gradient descent • 45</p><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Summary                                                51</h3><p style="text-indent: 0pt;text-align: left;"/><h2 style="padding-top: 13pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Chapter 3: A Tour of Machine Learning Classifiers Using Scikit-Learn      53</h2><h3 style="padding-top: 10pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Choosing a classification algorithm                                 53</h3><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">First steps with scikit-learn – training a perceptron                         54</h3><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Modeling class probabilities via logistic regression                         59</h3><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;line-height: 121%;text-align: left;">Logistic regression and conditional probabilities • 60 Learning the model weights via the logistic loss function • 63</p><p style="padding-left: 14pt;text-indent: 0pt;line-height: 121%;text-align: left;">Converting an Adaline implementation into an algorithm for logistic regression • 66 Training a logistic regression model with scikit-learn • 70</p><p style="padding-left: 14pt;text-indent: 0pt;text-align: left;">Tackling overfitting via regularization • 73</p><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Maximum margin classification with support vector machines                  76</h3><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Maximum margin intuition • 77</p><p style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;line-height: 121%;text-align: left;">Dealing with a nonlinearly separable case using slack variables • 77 Alternative implementations in scikit-learn • 79</p><h3 style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Solving nonlinear problems using a kernel SVM                          80</h3><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Kernel methods for linearly inseparable data • 80</p><p style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">Using the kernel trick to find separating hyperplanes in a high-dimensional space • 82</p><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Decision tree learning                                         86</h3><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;line-height: 121%;text-align: left;">Maximizing IG – getting the most bang for your buck • 88 Building a decision tree • 92</p><p style="padding-left: 14pt;text-indent: 0pt;text-align: left;">Combining multiple decision trees via random forests • 95</p><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">K-nearest neighbors – a lazy learning algorithm                           98</h3><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Summary                                               102</h3><p style="text-indent: 0pt;text-align: left;"/><h2 style="padding-top: 5pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">Chapter 4: Building Good Training Datasets – Data Preprocessing       105</h2><h3 style="padding-top: 9pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Dealing with missing data                                      105</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Identifying missing values in tabular data • 106</p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Eliminating training examples or features with missing values • 107 Imputing missing values • 108</p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;">Understanding the scikit-learn estimator API • 109</p><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Handling categorical data                                      111</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Categorical data encoding with pandas • 111 Mapping ordinal features • 111</p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;">Encoding class labels • 112</p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Performing one-hot encoding on nominal features • 113</p><p class="s1" style="padding-top: 2pt;padding-left: 49pt;text-indent: 0pt;text-align: left;">Optional: encoding ordinal features • 116</p><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Partitioning a dataset into separate training and test datasets                   117</h3><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Bringing features onto the same scale                                119</h3><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Selecting meaningful features                                    122</h3><p style="padding-top: 2pt;padding-left: 39pt;text-indent: 0pt;line-height: 121%;text-align: left;">L1 and L2 regularization as penalties against model complexity • 122 A geometric interpretation of L2 regularization • 123</p><p style="padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Sparse solutions with L1 regularization • 125 Sequential feature selection algorithms • 128</p><h3 style="padding-left: 30pt;text-indent: 0pt;text-align: left;">Assessing feature importance with random forests                        134</h3><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Summary                                               137</h3><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-bottom: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Chapter 5: Compressing Data via Dimensionality Reduction          139</h2><p style="padding-left: 31pt;text-indent: 0pt;line-height: 1pt;text-align: left;"/><h3 style="padding-top: 6pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Unsupervised dimensionality reduction via principal component analysis           139</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">The main steps in principal component analysis • 140 Extracting the principal components step by step • 142 Total and explained variance • 144</p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;">Feature transformation • 146</p><p style="padding-top: 2pt;padding-left: 39pt;text-indent: 0pt;line-height: 121%;text-align: left;">Principal component analysis in scikit-learn • 149 Assessing feature contributions • 152</p><h3 style="padding-left: 31pt;text-indent: 0pt;text-align: left;">Supervised data compression via linear discriminant analysis                  154</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Principal component analysis versus linear discriminant analysis • 154 The inner workings of linear discriminant analysis • 156</p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;">Computing the scatter matrices • 156</p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Selecting linear discriminants for the new feature subspace • 158 Projecting examples onto the new feature space • 161</p><p style="padding-top: 4pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">LDA via scikit-learn • 162</p><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Nonlinear dimensionality reduction and visualization                      163</h3><p style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">Why consider nonlinear dimensionality reduction? • 164</p><p style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">Visualizing data via t-distributed stochastic neighbor embedding • 165</p><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Summary                                               169</h3><h2 style="padding-top: 13pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Chapter 6: Learning Best Practices for Model Evaluation and</h2><p style="text-indent: 0pt;text-align: left;"/><h2 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Hyperparameter Tuning                                171</h2><h3 style="padding-top: 10pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Streamlining workfiows with pipelines                              171</h3><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;line-height: 121%;text-align: left;">Loading the Breast Cancer Wisconsin dataset • 172 Combining transformers and estimators in a pipeline • 173</p><h3 style="padding-left: 5pt;text-indent: 0pt;text-align: left;">Using k-fold cross-validation to assess model performance                    175</h3><p style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">The holdout method • 175</p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">K-fold cross-validation • 176</p><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Debugging algorithms with learning and validation curves                   180</h3><p style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;line-height: 121%;text-align: left;">Diagnosing bias and variance problems with learning curves • 180 Addressing over- and underfitting with validation curves • 183</p><h3 style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Fine-tuning machine learning models via grid search                       185</h3><p style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">Tuning hyperparameters via grid search • 186</p><p style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;line-height: 121%;text-align: left;">Exploring hyperparameter configurations more widely with randomized search • 187 More resource-efficient hyperparameter search with successive halving • 189 Algorithm selection with nested cross-validation • 191</p><h3 style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Looking at different performance evaluation metrics                       193</h3><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Reading a confusion matrix • 193</p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;line-height: 121%;text-align: left;">Optimizing the precision and recall of a classification model • 195 Plotting a receiver operating characteristic • 198</p><p style="padding-left: 15pt;text-indent: 0pt;line-height: 121%;text-align: left;">Scoring metrics for multiclass classification • 200 Dealing with class imbalance • 201</p><h3 style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Summary                                               203</h3><p style="text-indent: 0pt;text-align: left;"/><h2 style="padding-top: 13pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Chapter 7: Combining Different Models for Ensemble Learning         205</h2><h3 style="padding-top: 10pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Learning with ensembles                                      205</h3><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Combining classifiers via majority vote                              209</h3><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Implementing a simple majority vote classifier • 209</p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;line-height: 121%;text-align: left;">Using the majority voting principle to make predictions • 214 Evaluating and tuning the ensemble classifier • 217</p><h3 style="padding-top: 4pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Bagging – building an ensemble of classifiers from bootstrap samples             223</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Bagging in a nutshell • 224</p><p style="padding-top: 2pt;padding-left: 39pt;text-indent: 0pt;text-align: left;">Applying bagging to classify examples in the Wine dataset • 225</p><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Leveraging weak learners via adaptive boosting                          229</h3><p style="padding-top: 2pt;padding-left: 39pt;text-indent: 0pt;line-height: 121%;text-align: left;">How adaptive boosting works • 229 Applying AdaBoost using scikit-learn • 233</p><h3 style="padding-left: 31pt;text-indent: 0pt;text-align: left;">Gradient boosting – training an ensemble based on loss gradients                237</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Comparing AdaBoost with gradient boosting • 237 Outlining the general gradient boosting algorithm • 237</p><p style="padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Explaining the gradient boosting algorithm for classification • 239 Illustrating gradient boosting for classification • 241</p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;">Using XGBoost • 243</p><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Summary                                               245</h3><p style="text-indent: 0pt;text-align: left;"/><h2 style="padding-top: 13pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Chapter 8: Applying Machine Learning to Sentiment Analysis          247</h2><h3 style="padding-top: 10pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Preparing the IMDb movie review data for text processing                    247</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Obtaining the movie review dataset • 248</p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Preprocessing the movie dataset into a more convenient format • 248</p><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Introducing the bag-of-words model                                250</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Transforming words into feature vectors • 250</p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Assessing word relevancy via term frequency-inverse document frequency • 252 Cleaning text data • 254</p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;">Processing documents into tokens • 256</p><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Training a logistic regression model for document classification                 258</h3><h3 style="padding-top: 2pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">Working with bigger data – online algorithms and out-of-core learning             260</h3><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Topic modeling with latent Dirichlet allocation                          264</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Decomposing text documents with LDA • 264 LDA with scikit-learn • 265</p><h3 style="padding-left: 31pt;text-indent: 0pt;text-align: left;">Summary                                               268</h3><p class="s2" style="padding-top: 13pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">Chapter 9: Predicting Continuous Target Variables with Regression Analysis  269</p><h3 style="padding-top: 10pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Introducing linear regression                                    269</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Simple linear regression • 270 Multiple linear regression • 271</p><h3 style="padding-left: 31pt;text-indent: 0pt;text-align: left;">Exploring the Ames Housing dataset                                272</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Loading the Ames Housing dataset into a DataFrame • 272</p><p style="padding-top: 4pt;padding-left: 15pt;text-indent: 0pt;line-height: 121%;text-align: left;">Visualizing the important characteristics of a dataset • 274 Looking at relationships using a correlation matrix • 276</p><h3 style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Implementing an ordinary least squares linear regression model                278</h3><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;line-height: 121%;text-align: left;">Solving regression for regression parameters with gradient descent • 278 Estimating the coefficient of a regression model via scikit-learn • 283</p><h3 style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Fitting a robust regression model using RANSAC                         285</h3><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Evaluating the performance of linear regression models                     288</h3><h3 style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Using regularized methods for regression                             292</h3><h3 style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Turning a linear regression model into a curve – polynomial regression             294</h3><p style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">Adding polynomial terms using scikit-learn • 294</p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Modeling nonlinear relationships in the Ames Housing dataset • 297</p><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Dealing with nonlinear relationships using random forests                   299</h3><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;line-height: 121%;text-align: left;">Decision tree regression • 300 Random forest regression • 301</p><h3 style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Summary                                               304</h3><p style="text-indent: 0pt;text-align: left;"/><h2 style="padding-top: 13pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Chapter 10: Working with Unlabeled Data – Clustering Analysis        305</h2><h3 style="padding-top: 10pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Grouping objects by similarity using k-means                           305</h3><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">k-means clustering using scikit-learn • 305</p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;line-height: 121%;text-align: left;">A smarter way of placing the initial cluster centroids using k-means++ • 310 Hard versus soft clustering • 311</p><p style="padding-left: 15pt;text-indent: 0pt;line-height: 121%;text-align: left;">Using the elbow method to find the optimal number of clusters • 313 Quantifying the quality of clustering via silhouette plots • 314</p><h3 style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Organizing clusters as a hierarchical tree                             319</h3><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Grouping clusters in a bottom-up fashion • 320</p><p style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;line-height: 121%;text-align: left;">Performing hierarchical clustering on a distance matrix • 321 Attaching dendrograms to a heat map • 325</p><p style="padding-left: 14pt;text-indent: 0pt;text-align: left;">Applying agglomerative clustering via scikit-learn • 327</p><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Locating regions of high density via DBSCAN                           328</h3><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Summary                                               334</h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">Chapter 11: Implementing a Multilayer Artificial Neural Network from Scratch  335</p><h3 style="padding-top: 10pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Modeling complex functions with artificial neural networks                   335</h3><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Single-layer neural network recap • 337</p><p style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;line-height: 121%;text-align: left;">Introducing the multilayer neural network architecture • 338 Activating a neural network via forward propagation • 340</p><h3 style="padding-top: 4pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Classifying handwritten digits                                   343</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Obtaining and preparing the MNIST dataset • 343 Implementing a multilayer perceptron • 347 Coding the neural network training loop • 352 Evaluating the neural network performance • 357</p><h3 style="padding-left: 31pt;text-indent: 0pt;text-align: left;">Training an artificial neural network                                360</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Computing the loss function • 360</p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Developing your understanding of backpropagation • 362 Training neural networks via backpropagation • 363</p><h3 style="padding-left: 30pt;text-indent: 0pt;text-align: left;">About convergence in neural networks                               367</h3><h3 style="padding-top: 2pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">A few last words about the neural network implementation                   368</h3><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Summary                                               368</h3><p style="text-indent: 0pt;text-align: left;"/><h2 style="padding-top: 13pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">Chapter 12: Parallelizing Neural Network Training with PyTorch        369</h2><h3 style="padding-top: 10pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">PyTorch and training performance                                 369</h3><p style="padding-top: 2pt;padding-left: 39pt;text-indent: 0pt;line-height: 121%;text-align: left;">Performance challenges • 369 What is PyTorch? • 371</p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;">How we will learn PyTorch • 372</p><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">First steps with PyTorch                                       372</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Installing PyTorch • 372</p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Creating tensors in PyTorch • 373</p><p style="padding-top: 2pt;padding-left: 39pt;text-indent: 0pt;line-height: 121%;text-align: left;">Manipulating the data type and shape of a tensor • 374 Applying mathematical operations to tensors • 375 Split, stack, and concatenate tensors • 376</p><h3 style="padding-left: 31pt;text-indent: 0pt;text-align: left;">Building input pipelines in PyTorch                                378</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Creating a PyTorch DataLoader from existing tensors • 378 Combining two tensors into a joint dataset • 379</p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;">Shuffie, batch, and repeat • 380</p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Creating a dataset from files on your local storage disk • 382</p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Fetching available datasets from the torchvision.datasets library • 386</p><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Building an NN model in PyTorch                                 389</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">The PyTorch neural network module (torch.nn) • 390 Building a linear regression model • 390</p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;">Model training via the torch.nn and torch.optim modules • 394</p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Building a multilayer perceptron for classifying flowers in the Iris dataset • 395 Evaluating the trained model on the test dataset • 398</p><p style="padding-top: 4pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Saving and reloading the trained model • 399</p><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Choosing activation functions for multilayer neural networks                  400</h3><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Logistic function recap • 400</p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;line-height: 121%;text-align: left;">Estimating class probabilities in multiclass classification via the softmax function • 402 Broadening the output spectrum using a hyperbolic tangent • 403</p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;">Rectified linear unit activation • 405</p><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Summary                                               406</h3><p style="text-indent: 0pt;text-align: left;"/><h2 style="padding-top: 13pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Chapter 13: Going Deeper – The Mechanics of PyTorch             409</h2><h3 style="padding-top: 10pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">The key features of PyTorch                                     410</h3><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">PyTorch’s computation graphs                                   410</h3><p style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;line-height: 121%;text-align: left;">Understanding computation graphs • 410 Creating a graph in PyTorch • 411</p><h3 style="padding-left: 6pt;text-indent: 0pt;text-align: left;">PyTorch tensor objects for storing and updating model parameters               412</h3><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Computing gradients via automatic differentiation                        415</h3><p style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;line-height: 121%;text-align: left;">Computing the gradients of the loss with respect to trainable variables • 415 Understanding automatic differentiation • 416</p><p style="padding-left: 14pt;text-indent: 0pt;text-align: left;">Adversarial examples • 416</p><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Simplifying implementations of common architectures via the torch.nn module        417</h3><p style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;line-height: 121%;text-align: left;">Implementing models based on nn.Sequential • 417 Choosing a loss function • 418</p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;">Solving an XOR classification problem • 419</p><p style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;line-height: 121%;text-align: left;">Making model building more flexible with nn.Module • 424 Writing custom layers in PyTorch • 426</p><h3 style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Project one – predicting the fuel efficiency of a car                         431</h3><p style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;line-height: 121%;text-align: left;">Working with feature columns • 431 Training a DNN regression model • 435</p><h3 style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Project two – classifying MNIST handwritten digits                        436</h3><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Higher-level PyTorch APIs: a short introduction to PyTorch-Lightning             439</h3><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;line-height: 121%;text-align: left;">Setting up the PyTorch Lightning model • 440 Setting up the data loaders for Lightning • 443</p><p style="padding-left: 15pt;text-indent: 0pt;line-height: 121%;text-align: left;">Training the model using the PyTorch Lightning Trainer class • 444 Evaluating the model using TensorBoard • 445</p><h3 style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Summary                                               449</h3><p class="s2" style="padding-top: 5pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Chapter 14: Classifying Images with Deep Convolutional Neural Networks &nbsp; 451</p><h3 style="padding-top: 9pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">The building blocks of CNNs                                    451</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Understanding CNNs and feature hierarchies • 452 Performing discrete convolutions • 454</p><p class="s1" style="padding-left: 49pt;text-indent: 0pt;text-align: left;">Discrete convolutions in one dimension • 454</p><p class="s1" style="padding-top: 2pt;padding-left: 49pt;text-indent: 0pt;line-height: 121%;text-align: left;">Padding inputs to control the size of the output feature maps • 457 Determining the size of the convolution output • 458</p><p class="s1" style="padding-left: 49pt;text-indent: 0pt;text-align: left;">Performing a discrete convolution in 2D • 459</p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Subsampling layers • 463</p><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Putting everything together – implementing a CNN                        464</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Working with multiple input or color channels • 464 Regularizing an NN with L2 regularization and dropout • 467 Loss functions for classification • 471</p><h3 style="padding-left: 31pt;text-indent: 0pt;text-align: left;">Implementing a deep CNN using PyTorch                             473</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">The multilayer CNN architecture • 473 Loading and preprocessing the data • 474</p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;">Implementing a CNN using the torch.nn module • 476</p><p class="s1" style="padding-top: 2pt;padding-left: 49pt;text-indent: 0pt;line-height: 121%;text-align: left;">Configuring CNN layers in PyTorch • 476 Constructing a CNN in PyTorch • 477</p><h3 style="padding-left: 31pt;text-indent: 0pt;text-align: left;">Smile classification from face images using a CNN                        482</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Loading the CelebA dataset • 483</p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Image transformation and data augmentation • 484 Training a CNN smile classifier • 490</p><h3 style="padding-left: 31pt;text-indent: 0pt;text-align: left;">Summary                                               497</h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 30pt;text-indent: 0pt;text-align: left;">Chapter 15: Modeling Sequential Data Using Recurrent Neural Networks &nbsp; 499</p><h3 style="padding-top: 10pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Introducing sequential data                                     499</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Modeling sequential data – order matters • 500 Sequential data versus time series data • 500 Representing sequences • 500</p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;">The different categories of sequence modeling • 501</p><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">RNNs for modeling sequences                                   502</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Understanding the dataflow in RNNs • 502 Computing activations in an RNN • 504</p><p style="padding-top: 4pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Hidden recurrence versus output recurrence • 506</p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;line-height: 121%;text-align: left;">The challenges of learning long-range interactions • 509 Long short-term memory cells • 511</p><h3 style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Implementing RNNs for sequence modeling in PyTorch                     513</h3><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Project one – predicting the sentiment of IMDb movie reviews • 513</p><p class="s1" style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;line-height: 121%;text-align: left;">Preparing the movie review data • 513 Embedding layers for sentence encoding • 517 Building an RNN model • 520</p><p class="s1" style="padding-left: 23pt;text-indent: 0pt;text-align: left;">Building an RNN model for the sentiment analysis task • 521</p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Project two – character-level language modeling in PyTorch • 525</p><p class="s1" style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;">Preprocessing the dataset • 526</p><p class="s1" style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;line-height: 121%;text-align: left;">Building a character-level RNN model • 531 Evaluation phase – generating new text passages • 533</p><h3 style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Summary                                               537</h3><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-left: 5pt;text-indent: 0pt;text-align: left;">Chapter 16: Transformers – Improving Natural Language Processing</h2><h2 style="padding-top: 2pt;padding-bottom: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">with Attention Mechanisms                              539</h2><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"/><h3 style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Adding an attention mechanism to RNNs                             540</h3><p style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;line-height: 121%;text-align: left;">Attention helps RNNs with accessing information • 540 The original attention mechanism for RNNs • 542 Processing the inputs using a bidirectional RNN • 543 Generating outputs from context vectors • 543 Computing the attention weights • 544</p><h3 style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Introducing the self-attention mechanism                             544</h3><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Starting with a basic form of self-attention • 545</p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Parameterizing the self-attention mechanism: scaled dot-product attention • 549</p><h3 style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Attention is all we need: introducing the original transformer architecture           552</h3><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Encoding context embeddings via multi-head attention • 554</p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;line-height: 121%;text-align: left;">Learning a language model: decoder and masked multi-head attention • 558 Implementation details: positional encodings and layer normalization • 559</p><h3 style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Building large-scale language models by leveraging unlabeled data               561</h3><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;line-height: 121%;text-align: left;">Pre-training and fine-tuning transformer models • 561 Leveraging unlabeled data with GPT • 563</p><p style="padding-left: 14pt;text-indent: 0pt;line-height: 121%;text-align: left;">Using GPT-2 to generate new text • 566 Bidirectional pre-training with BERT • 569 The best of both worlds: BART • 572</p><h3 style="padding-top: 4pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Fine-tuning a BERT model in PyTorch                               574</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Loading the IMDb movie review dataset • 575 Tokenizing the dataset • 577</p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;">Loading and fine-tuning a pre-trained BERT model • 578</p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Fine-tuning a transformer more conveniently using the Trainer API • 582</p><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Summary                                               586</h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 31pt;text-indent: 0pt;text-align: left;">Chapter 17: Generative Adversarial Networks for Synthesizing New Data &nbsp; 589</p><h3 style="padding-top: 10pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Introducing generative adversarial networks                           589</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Starting with autoencoders • 590</p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Generative models for synthesizing new data • 592 Generating new samples with GANs • 593</p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;">Understanding the loss functions of the generator and discriminator networks in a GAN model • 594</p><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Implementing a GAN from scratch                                 596</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Training GAN models on Google Colab • 596</p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Implementing the generator and the discriminator networks • 600 Defining the training dataset • 604</p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;">Training the GAN model • 605</p><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Improving the quality of synthesized images using a convolutional and Wasserstein GAN   612</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Transposed convolution • 612 Batch normalization • 614</p><p style="padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Implementing the generator and discriminator • 616 Dissimilarity measures between two distributions • 624 Using EM distance in practice for GANs • 627</p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;">Gradient penalty • 628</p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Implementing WGAN-GP to train the DCGAN model • 629 Mode collapse • 633</p><h3 style="padding-left: 31pt;text-indent: 0pt;text-align: left;">Other GAN applications                                       635</h3><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Summary                                               635</h3><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-left: 31pt;text-indent: 0pt;text-align: left;">Chapter 18: Graph Neural Networks for Capturing Dependencies</h2><h2 style="padding-top: 2pt;padding-bottom: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">in Graph Structured Data                               637</h2><p style="padding-left: 31pt;text-indent: 0pt;line-height: 1pt;text-align: left;"/><h3 style="padding-top: 6pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Introduction to graph data                                      638</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;">Undirected graphs • 638 Directed graphs • 639</p><p style="padding-top: 4pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Labeled graphs • 640</p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Representing molecules as graphs • 640</p><h3 style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Understanding graph convolutions                                 641</h3><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;line-height: 121%;text-align: left;">The motivation behind using graph convolutions • 641 Implementing a basic graph convolution • 644</p><h3 style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Implementing a GNN in PyTorch from scratch                           648</h3><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Defining the NodeNetwork model • 649</p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Coding the NodeNetwork’s graph convolution layer • 650</p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;line-height: 121%;text-align: left;">Adding a global pooling layer to deal with varying graph sizes • 652 Preparing the DataLoader • 655</p><p style="padding-left: 14pt;text-indent: 0pt;text-align: left;">Using the NodeNetwork to make predictions • 658</p><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Implementing a GNN using the PyTorch Geometric library                   659</h3><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Other GNN layers and recent developments                            665</h3><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;line-height: 121%;text-align: left;">Spectral graph convolutions • 665 Pooling • 667</p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;">Normalization • 668</p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Pointers to advanced graph neural network literature • 669</p><h3 style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Summary                                               671</h3><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-left: 5pt;text-indent: 0pt;text-align: left;">Chapter 19: Reinforcement Learning for Decision Making in</h2><h2 style="padding-top: 2pt;padding-bottom: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Complex Environments                                673</h2><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"/><h3 style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Introduction – learning from experience                             674</h3><p style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">Understanding reinforcement learning • 674</p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Defining the agent-environment interface of a reinforcement learning system • 675</p><h3 style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">The theoretical foundations of RL                                  676</h3><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Markov decision processes • 677</p><p class="s1" style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;line-height: 121%;text-align: left;">The mathematical formulation of Markov decision processes • 677 Visualization of a Markov process • 679</p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;">Episodic versus continuing tasks • 679</p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">RL terminology: return, policy, and value function • 680</p><p class="s1" style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;line-height: 121%;text-align: left;">The return • 680 Policy • 682</p><p class="s1" style="padding-left: 23pt;text-indent: 0pt;text-align: left;">Value function • 682</p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Dynamic programming using the Bellman equation • 684</p><h3 style="padding-top: 4pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Reinforcement learning algorithms                                 684</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Dynamic programming • 685</p><p class="s1" style="padding-top: 2pt;padding-left: 49pt;text-indent: 0pt;line-height: 121%;text-align: left;">Policy evaluation – predicting the value function with dynamic programming • 686 Improving the policy using the estimated value function • 686</p><p class="s1" style="padding-left: 48pt;text-indent: 0pt;line-height: 121%;text-align: left;">Policy iteration • 687 Value iteration • 687</p><p style="padding-left: 48pt;text-indent: -8pt;line-height: 121%;text-align: left;">Reinforcement learning with Monte Carlo • 687 <i>State-value function estimation using MC • 688 Action-value function estimation using MC • 688 Finding an optimal policy using MC control • 688</i></p><p class="s1" style="padding-left: 49pt;text-indent: 0pt;text-align: left;">Policy improvement – computing the greedy policy from the action-value function • 689</p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Temporal difference learning • 689</p><p class="s1" style="padding-top: 2pt;padding-left: 48pt;text-indent: 0pt;text-align: left;">TD prediction • 689</p><p class="s1" style="padding-top: 2pt;padding-left: 49pt;text-indent: 0pt;text-align: left;">On-policy TD control (SARSA) • 691</p><p class="s1" style="padding-top: 2pt;padding-left: 49pt;text-indent: 0pt;text-align: left;">Off-policy TD control (Q-learning) • 691</p><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Implementing our first RL algorithm                                691</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Introducing the OpenAI Gym toolkit • 692</p><p class="s1" style="padding-top: 2pt;padding-left: 48pt;text-indent: 0pt;line-height: 121%;text-align: left;">Working with the existing environments in OpenAI Gym • 692 A grid world example • 694</p><p class="s1" style="padding-left: 49pt;text-indent: 0pt;text-align: left;">Implementing the grid world environment in OpenAI Gym • 694</p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Solving the grid world problem with Q-learning • 701</p><h3 style="padding-top: 2pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">A glance at deep Q-learning                                     706</h3><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Training a DQN model according to the Q-learning algorithm • 706</p><p class="s1" style="padding-top: 2pt;padding-left: 49pt;text-indent: 0pt;text-align: left;">Replay memory • 707</p><p class="s1" style="padding-top: 2pt;padding-left: 49pt;text-indent: 0pt;text-align: left;">Determining the target values for computing the loss • 708</p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Implementing a deep Q-learning algorithm • 710</p><h3 style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Chapter and book summary                                     714</h3><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-bottom: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Other Books You May Enjoy                             719</h2><p style="padding-left: 31pt;text-indent: 0pt;line-height: 1pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-bottom: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Index                                           723</h2><p style="padding-left: 31pt;text-indent: 0pt;line-height: 1pt;text-align: left;"/></body></html>
